# GAN-tionary (EYSIP 2022)

<br>

## DVQA

---
<br>

**Synthesis Reports:**

**CNNs** ~ <https://samya-ravenxi.notion.site/Convolutional-Neural-Networks-CNNs-cb2ca9e7765b46f4a6437af0540f7abd>

**RNNs**  ~ <https://samya-ravenxi.notion.site/Recurrent-Neural-Networks-RNNs-e904a4e282bf4141865204a47e01521f>

**LSTMs** ~ <https://samya-ravenxi.notion.site/Long-Short-Term-Memory-Networks-LSTMs-5f1ffa44545641dfbe4c7e54ca85c6b4>

**Attentions** ~ <https://samya-ravenxi.notion.site/Attention-Models-6f1b3e2c50fe4ab38264db9b01f6c578>

**Transformers** ~ <https://samya-ravenxi.notion.site/Transformers-b31498d91b7c4198a29aaae5059e674f>

**GANs** ~ <https://samya-ravenxi.notion.site/Generative-Adversarial-Networks-GANs-5a6b5dae6268418f988762f23c4996b3>

**Kaggle Notebook for GANs** ~ <https://www.kaggle.com/code/samyabose/gans-for-generating-weakly-reflexive-relations?kernelSessionId=97919755>

---
<br>


**Datasets:**

**DVQA** ~ <https://drive.google.com/drive/folders/1EzduOrm1izxP2DlqZs89OD7vYIjZULgm?usp=sharing**>

**VQA Abstract** ~ <https://drive.google.com/drive/folders/1vIbuQACWagN0j3qoeSuDUdVKUMvBHCwv?usp=sharing>

[Containing model weights, preprocessed data, performance measure etc.]

---
<br>

**Notebooks:**

**SAN-VQA Demonstration** ~ <https://colab.research.google.com/drive/1gLexF10EtafVxWEYHHp8Lv53H2WEDC_R?usp=sharing>

**SAN-VQA on VQA** ~ <https://colab.research.google.com/drive/12MYBlhD2hUPK0GZ8FjmIqGvleph_jf1U?usp=sharing>

**Preprocessing of DVQA** ~ <https://colab.research.google.com/drive/1G_TmUqKx3bXukhUQKn8KK948jm5JcGPO?usp=sharing>

**SAN-VQA on DVQA** ~ <https://colab.research.google.com/drive/1DMTx0lkz2UvyBEFyO2mBn5veEppvY_rf?usp=sharing>

**Binary Classifier** ~ <https://colab.research.google.com/drive/1C2tyshkPEgaPdLW_9V5IyQ2shi1TRrGV?usp=sharing>

**Regression Model and OCR** ~ <https://colab.research.google.com/drive/1dtjMx0FjDGG8FV5XyGNwKFPwkWRFkuUq?usp=sharing>

**Performance Analysis of PReFIL** ~
<https://colab.research.google.com/drive/13bcnxXMIBO2ne69uJwhseFhDkIv1uxtD?usp=sharing>


<br>
<br>

## pix2pix

---
<br>

**Datasets:**

**Link for all datasets -** <http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/>

**Drive link for all things necessary for pix2pix -** <https://drive.google.com/drive/folders/1mm3zxxlA_KwEdM3UbYDpAbQLwnEkBQsA?usp=sharing>

---
<br>


**Notebooks:**

**Demonstration of pix2pix -** <https://colab.research.google.com/drive/1HBbk8KdCNuZpOeNNKe-HDspD51Rvd3hK?usp=sharing>

**Detailed Study on pix2ix -** <https://colab.research.google.com/drive/1i1juRFut85oLUYAlC3E0-k1XKQn6M31N?usp=sharing>